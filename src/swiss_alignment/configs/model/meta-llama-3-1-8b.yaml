# @package _global_

model_args:
  torch_dtype: bfloat16
  model_name_or_path: meta-llama/Llama-3.1-8B
  attn_implementation: flash_attention_2
  trust_remote_code: true
  use_peft: false

tokenizer_args:
  tokenizer_name_or_path: meta-llama/Llama-3.1-8B
  trust_remote_code: true
  model_pad_token_id: 0  # <unk>
  model_eos_token_id: null # keep default. (128001)