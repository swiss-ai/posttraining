sbatch -N 64 -p large512 -t 24:00:00 -o reproducibility-scripts/sft/final/ademamix-2025-08-12-01-00/out/final-run/Apertus8B-tokens10.2T-it2059810-newcooldown-olmo2-with-tools-ln-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix.out -e reproducibility-scripts/sft/final/ademamix-2025-08-12-01-00/out/final-run/Apertus8B-tokens10.2T-it2059810-newcooldown-olmo2-with-tools-ln-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix.err ./cscs-shared-submit-scripts/recursive-unattended-accelerate.sh -m post_training.train_sft dataset=olmo2-with-tools-ln model=apertus-8b model_args.model_name_or_path=/capstor/store/cscs/swissai/infra01/pretrain-checkpoints/apertus/Apertus8B-tokens10.2T-it2059810-newcooldown tokenizer_args.tokenizer_name_or_path=/capstor/store/cscs/swissai/infra01/pretrain-checkpoints/apertus/Apertus8B-tokens10.2T-it2059810-newcooldown trainer=plw accelerate_config=src/post_training/configs/accelerate/ds-zero2.yaml plw_args.prompt_loss_weight=0.0 training_args.gradient_accumulation_steps=1 training_args.per_device_train_batch_size=2 training_args.optim=ademamix training_args.learning_rate=5e-06 training_args.max_grad_norm=1.0 tokenizer_args.chat_template_name=apertus training_args.num_train_epochs=1 artifacts_subdir=shared job_subdir=final-run/Apertus8B-tokens10.2T-it2059810-newcooldown-olmo2-with-tools-ln-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix wandb.run_name=final-run/Apertus8B-tokens10.2T-it2059810-newcooldown-olmo2-with-tools-ln-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix wandb.tags=[prod,plw,default,final-run] resuming.resume=True 
