sbatch -N 8 -p normal -t 12:00:00 -o reproducibility-scripts/sft/final/ademamix-qa-2025-08-30-23-00/out/qa-run/Apertus-8B-sft-mixture-8e-aligned-branding-qa-bs128-lr5e-07-maxgnorm1.0-epochs6-ademamix-apertus-pad-left.out -e reproducibility-scripts/sft/final/ademamix-qa-2025-08-30-23-00/out/qa-run/Apertus-8B-sft-mixture-8e-aligned-branding-qa-bs128-lr5e-07-maxgnorm1.0-epochs6-ademamix-apertus-pad-left.err ./cscs-shared-submit-scripts/unattended-accelerate.sh -m post_training.train_sft dataset=branding-qa model=apertus-8b model_args.model_name_or_path=/capstor/store/cscs/swissai/infra01/swiss-alignment/checkpoints/Apertus-8B-sft-mixture-8e-aligned tokenizer_args.tokenizer_name_or_path=/capstor/store/cscs/swissai/infra01/swiss-alignment/checkpoints/Apertus-8B-sft-mixture-8e-aligned trainer=plw accelerate_config=src/post_training/configs/accelerate/ds-zero2.yaml plw_args.prompt_loss_weight=0.0 training_args.gradient_accumulation_steps=2 training_args.per_device_train_batch_size=2 training_args.optim=ademamix training_args.learning_rate=5e-07 training_args.lr_scheduler_type=constant training_args.warmup_ratio=0 training_args.warmup_steps=0 training_args.max_grad_norm=1.0 training_args.dataset_num_proc=1 tokenizer_args.chat_template_name=apertus tokenizer_args.model_eos_token_id=68 tokenizer_args.padding_side=left training_args.num_train_epochs=6 training_args.save_strategy=epoch training_args.save_total_limit=20 artifacts_subdir=shared job_subdir=qa-run/Apertus-8B-sft-mixture-8e-aligned-branding-qa-bs128-lr5e-07-maxgnorm1.0-epochs6-ademamix-apertus-pad-left wandb.run_name=qa-run/Apertus-8B-sft-mixture-8e-aligned-branding-qa-bs128-lr5e-07-maxgnorm1.0-epochs6-ademamix-apertus-pad-left wandb.tags=[prod,plw,default,qa-run] resuming.resume=True global_batch_size=128 num_nodes=8 
